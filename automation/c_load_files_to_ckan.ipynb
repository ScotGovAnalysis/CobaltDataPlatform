{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 10:00:30,482 - INFO - Starting file processing...\n",
      "2025-05-29 10:00:30,482 - INFO - Starting file processing...\n",
      "2025-05-29 10:00:30,484 - INFO - Found 1 files to process\n",
      "2025-05-29 10:00:30,484 - INFO - Found 1 files to process\n",
      "2025-05-29 10:00:30,506 - INFO - Detected encoding: ascii (confidence: 1.00) for ./Pending\\files\\Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:30,506 - INFO - Detected encoding: ascii (confidence: 1.00) for ./Pending\\files\\Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:30,506 - INFO - Detected encoding: ascii (confidence: 1.00) for ./Pending\\files\\Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:30,506 - INFO - Detected encoding: ascii (confidence: 1.00) for ./Pending\\files\\Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:30,533 - WARNING - Failed to read with ascii, trying fallback methods: 'ascii' codec can't decode byte 0xf2 in position 73819: ordinal not in range(128)\n",
      "2025-05-29 10:00:30,533 - WARNING - Failed to read with ascii, trying fallback methods: 'ascii' codec can't decode byte 0xf2 in position 73819: ordinal not in range(128)\n",
      "2025-05-29 10:00:30,533 - WARNING - Failed to read with ascii, trying fallback methods: 'ascii' codec can't decode byte 0xf2 in position 73819: ordinal not in range(128)\n",
      "2025-05-29 10:00:30,533 - WARNING - Failed to read with ascii, trying fallback methods: 'ascii' codec can't decode byte 0xf2 in position 73819: ordinal not in range(128)\n",
      "2025-05-29 10:00:30,627 - INFO - Successfully read CSV with ascii encoding and ignore error handling\n",
      "2025-05-29 10:00:30,627 - INFO - Successfully read CSV with ascii encoding and ignore error handling\n",
      "2025-05-29 10:00:30,627 - INFO - Successfully read CSV with ascii encoding and ignore error handling\n",
      "2025-05-29 10:00:30,627 - INFO - Successfully read CSV with ascii encoding and ignore error handling\n",
      "2025-05-29 10:00:31,640 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:31,640 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:31,640 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:31,640 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:32,980 - INFO - Dataset created: 2011_data_zone_archived_geographies\n",
      "2025-05-29 10:00:32,980 - INFO - Dataset created: 2011_data_zone_archived_geographies\n",
      "2025-05-29 10:00:32,980 - INFO - Dataset created: 2011_data_zone_archived_geographies\n",
      "2025-05-29 10:00:32,980 - INFO - Dataset created: 2011_data_zone_archived_geographies\n",
      "2025-05-29 10:00:33,442 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:33,442 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:33,442 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:33,442 - ERROR - HTTP Error [404] Attempt 1/3\n",
      "URL: http://35.177.24.156:5000/api/3/action/package_show\n",
      "Response: {\"help\": \"https://cobaltadmin.sgdatacatalogue.net/api/3/action/help_show?name=package_show\", \"error\": {\"__type\": \"Not Found Error\", \"message\": \"Not found\"}, \"success\": false}\n",
      "2025-05-29 10:00:35,780 - INFO - Resource created: Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:35,780 - INFO - Resource created: Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:35,780 - INFO - Resource created: Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:35,780 - INFO - Resource created: Data Zone Lookup - Archived Geographies.csv\n",
      "2025-05-29 10:00:35,812 - ERROR - Resource error: 'utf-8' codec can't decode byte 0xf2 in position 73819: invalid continuation byte\n",
      "2025-05-29 10:00:35,812 - ERROR - Resource error: 'utf-8' codec can't decode byte 0xf2 in position 73819: invalid continuation byte\n",
      "2025-05-29 10:00:35,812 - ERROR - Resource error: 'utf-8' codec can't decode byte 0xf2 in position 73819: invalid continuation byte\n",
      "2025-05-29 10:00:35,812 - ERROR - Resource error: 'utf-8' codec can't decode byte 0xf2 in position 73819: invalid continuation byte\n",
      "2025-05-29 10:00:35,817 - INFO - Report generated: ./Completed\\report\\report_20250529_100035.csv\n",
      "2025-05-29 10:00:35,817 - INFO - Report generated: ./Completed\\report\\report_20250529_100035.csv\n",
      "2025-05-29 10:00:35,817 - INFO - Report generated: ./Completed\\report\\report_20250529_100035.csv\n",
      "2025-05-29 10:00:35,817 - INFO - Report generated: ./Completed\\report\\report_20250529_100035.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import logging\n",
    "import json\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from Functions.utils import ConfigLoader, sanitize_name, setup_logging, load_metadata\n",
    "from Functions.ckan_manager import CKANManager\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "class FileProcessor:\n",
    "    def __init__(self):\n",
    "        self.config = ConfigLoader()\n",
    "        self.ckan = CKANManager(\n",
    "            self.config.ckan_api_url,\n",
    "            self.config.ckan_api_key\n",
    "        )\n",
    "        self.logger = setup_logging()\n",
    "\n",
    "    def _detect_encoding(self, file_path: str) -> str:\n",
    "        \"\"\"Detect file encoding using chardet.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                raw_data = file.read(100000)  # Read first 100KB for detection\n",
    "                result = chardet.detect(raw_data)\n",
    "                encoding = result['encoding']\n",
    "                confidence = result['confidence']\n",
    "                \n",
    "                self.logger.info(f\"Detected encoding: {encoding} (confidence: {confidence:.2f}) for {file_path}\")\n",
    "                \n",
    "                # If confidence is low, try common encodings\n",
    "                if confidence < 0.7:\n",
    "                    self.logger.warning(f\"Low confidence in encoding detection. Trying fallback encodings.\")\n",
    "                    return self._try_fallback_encodings(file_path)\n",
    "                \n",
    "                return encoding\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error detecting encoding for {file_path}: {str(e)}\")\n",
    "            return self._try_fallback_encodings(file_path)\n",
    "\n",
    "    def _try_fallback_encodings(self, file_path: str) -> str:\n",
    "        \"\"\"Try common encodings when detection fails or has low confidence.\"\"\"\n",
    "        encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    file.read(1000)  # Try to read first 1000 characters\n",
    "                self.logger.info(f\"Successfully validated encoding: {encoding} for {file_path}\")\n",
    "                return encoding\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # If all else fails, use latin-1 (it can decode any byte sequence)\n",
    "        self.logger.warning(f\"Using latin-1 as fallback encoding for {file_path}\")\n",
    "        return 'latin-1'\n",
    "\n",
    "    def _read_csv_with_encoding(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Read CSV file with proper encoding detection and handling.\"\"\"\n",
    "        encoding = self._detect_encoding(file_path)\n",
    "        \n",
    "        try:\n",
    "            # Try reading with detected encoding\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            self.logger.info(f\"Successfully read CSV with {encoding} encoding\")\n",
    "            return df\n",
    "        except UnicodeDecodeError as e:\n",
    "            self.logger.warning(f\"Failed to read with {encoding}, trying fallback methods: {str(e)}\")\n",
    "            \n",
    "            # Try with error handling\n",
    "            for error_handling in ['ignore', 'replace']:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding, encoding_errors=error_handling)\n",
    "                    self.logger.info(f\"Successfully read CSV with {encoding} encoding and {error_handling} error handling\")\n",
    "                    return df\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Last resort: try latin-1\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                self.logger.info(\"Successfully read CSV with latin-1 encoding as last resort\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Failed to read CSV with any encoding method: {str(e)}\")\n",
    "\n",
    "    def process_files(self) -> None:\n",
    "        \"\"\"Main processing loop with error containment.\"\"\"\n",
    "        files = self._get_pending_files()\n",
    "\n",
    "        for filename in files:\n",
    "            try:\n",
    "                success, message = self._process_single_file(filename)\n",
    "                self.ckan.add_to_report(filename, success, message)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Critical failure processing {filename}: {str(e)}\")\n",
    "                self.ckan.add_to_report(filename, False, \"Critical error\")\n",
    "\n",
    "        self.ckan.generate_report(self.config.completed_report_dir)\n",
    "\n",
    "    def _process_single_file(self, filename: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Process individual file with atomic operations.\"\"\"\n",
    "        file_path = os.path.join(self.config.pending_file_dir, filename)\n",
    "        completed_file_path = os.path.join(self.config.completed_file_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load data with proper encoding handling\n",
    "            if filename.endswith('.csv'):\n",
    "                df = self._read_csv_with_encoding(file_path)\n",
    "            elif filename.endswith('.xlsx'):\n",
    "                df = pd.read_excel(file_path)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported file type: {filename}\")\n",
    "                return False, \"Unsupported file type\"\n",
    "\n",
    "            # Step 2: Load metadata\n",
    "            metadata_template = load_metadata(self, filename)\n",
    "\n",
    "            # Step 3: Prepare dataset payload\n",
    "            dataset_payload = metadata_template.get(\"dataset\", {})\n",
    "            if not dataset_payload.get(\"name\"):\n",
    "                dataset_payload[\"name\"] = sanitize_name(os.path.splitext(filename)[0])\n",
    "            if not dataset_payload.get(\"title\"):\n",
    "                dataset_payload[\"title\"] = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Step 4: Create or update dataset\n",
    "            dataset_id = self.ckan.create_or_update_dataset(dataset_payload)\n",
    "            if not dataset_id:\n",
    "                return False, \"Failed to create/update dataset\"\n",
    "\n",
    "            # Step 5: Prepare resource payload\n",
    "            resource_payload = metadata_template.get(\"resource\", {})\n",
    "            resource_payload[\"name\"] = filename\n",
    "\n",
    "            # Step 6: Create or update resource\n",
    "            if not self.ckan.create_or_update_resource(dataset_id, resource_payload, file_path):\n",
    "                return False, \"Failed to create/update resource\"\n",
    "\n",
    "            # Step 7: Move processed file to completed directory\n",
    "            shutil.move(file_path, completed_file_path)\n",
    "\n",
    "            # Step 8: Move metadata file if it exists\n",
    "            if metadata_template:\n",
    "                metadata_matches = [\n",
    "                    f for f in os.listdir(self.config.pending_metadata_dir)\n",
    "                    if f.startswith(f\"metadata_{os.path.splitext(filename)[0]}\")\n",
    "                ]\n",
    "                if metadata_matches:\n",
    "                    latest_metadata = max(metadata_matches)\n",
    "                    metadata_path = os.path.join(self.config.pending_metadata_dir, latest_metadata)\n",
    "                    completed_metadata_path = os.path.join(self.config.completed_metadata_dir, latest_metadata)\n",
    "                    shutil.move(metadata_path, completed_metadata_path)\n",
    "                    self.logger.info(f\"Moved metadata file {latest_metadata} to {completed_metadata_path}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No metadata file found for {filename}\")\n",
    "            else:\n",
    "                self.logger.warning(\"No metadata template provided. Skipping metadata file movement.\")\n",
    "\n",
    "            self.logger.info(f\"Successfully processed {filename}\")\n",
    "            return True, \"Success\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error: {str(e)}\"\n",
    "            self.logger.error(f\"{error_msg} while processing {filename}\")\n",
    "            return False, error_msg\n",
    "\n",
    "    def _get_pending_files(self) -> List[str]:\n",
    "        \"\"\"Get list of files to process.\"\"\"\n",
    "        return [\n",
    "            f for f in os.listdir(self.config.pending_file_dir)\n",
    "            if os.path.isfile(os.path.join(self.config.pending_file_dir, f))\n",
    "            and f.endswith((\".csv\", \".xlsx\"))\n",
    "        ]\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the loader to process all pending files.\"\"\"\n",
    "    logger = setup_logging()\n",
    "    config = ConfigLoader()\n",
    "    ckan_manager = CKANManager(config.ckan_api_url, config.ckan_api_key)\n",
    "\n",
    "    logger.info(\"Starting file processing...\")\n",
    "    files = [\n",
    "        f for f in os.listdir(config.pending_file_dir)\n",
    "        if os.path.isfile(os.path.join(config.pending_file_dir, f)) and f.endswith((\".csv\", \".xlsx\"))\n",
    "    ]\n",
    "    logger.info(f\"Found {len(files)} files to process\")\n",
    "\n",
    "    processor = FileProcessor()\n",
    "    processor.process_files()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
