{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 20:38:20,606 - ckan_loader - INFO - Attempting to load ./Pending\\files\\Bikeability Scotland - Schools delivering Level 1 and_or Level 2 - 2019_20.csv with encoding: utf-8\n",
      "2025-02-12 20:38:20,606 - ckan_loader - INFO - Attempting to load ./Pending\\files\\Bikeability Scotland - Schools delivering Level 1 and_or Level 2 - 2019_20.csv with encoding: utf-8\n",
      "2025-02-12 20:38:20,617 - ckan_loader - INFO - Successfully loaded ./Pending\\files\\Bikeability Scotland - Schools delivering Level 1 and_or Level 2 - 2019_20.csv with encoding: utf-8\n",
      "2025-02-12 20:38:20,617 - ckan_loader - INFO - Successfully loaded ./Pending\\files\\Bikeability Scotland - Schools delivering Level 1 and_or Level 2 - 2019_20.csv with encoding: utf-8\n",
      "2025-02-12 20:38:20,618 - ckan_loader - ERROR - Processing failed: PostgreSQL load failed: name 'psycopg2' is not defined\n",
      "2025-02-12 20:38:20,618 - ckan_loader - ERROR - Processing failed: PostgreSQL load failed: name 'psycopg2' is not defined\n",
      "2025-02-12 20:38:20,618 - ckan_loader - ERROR - Failed to generate report: name 'csv' is not defined\n",
      "2025-02-12 20:38:20,618 - ckan_loader - ERROR - Failed to generate report: name 'csv' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import logging\n",
    "import json\n",
    "from typing import Tuple, Optional, Dict\n",
    "from Functions.utils import ConfigLoader, sanitize_name, setup_logging\n",
    "from Functions.ckan_manager import CKANManager\n",
    "from typing import List, Dict, Optional, Any  # Add this import\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "class FileProcessor:\n",
    "    def __init__(self):\n",
    "        self.config = ConfigLoader()\n",
    "        self.ckan = CKANManager(\n",
    "            self.config.ckan_api_url,\n",
    "            self.config.ckan_api_key\n",
    "        )\n",
    "        self.logger = setup_logging()\n",
    "\n",
    "    def process_files(self) -> None:\n",
    "        \"\"\"Main processing loop with error containment.\"\"\"\n",
    "        files = self._get_pending_files()\n",
    "        \n",
    "        for filename in files:\n",
    "            try:\n",
    "                success, message = self._process_single_file(filename)\n",
    "                self.ckan.add_to_report(filename, success, message)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Critical failure processing {filename}: {str(e)}\")\n",
    "                self.ckan.add_to_report(filename, False, \"Critical error\")\n",
    "\n",
    "        self.ckan.generate_report(self.config.completed_report_dir)\n",
    "\n",
    "    def _process_single_file(self, filename: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Process individual file with atomic operations.\"\"\"\n",
    "        file_path = os.path.join(self.config.pending_file_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load data\n",
    "            df = self._load_data(file_path)  # Only pass file_path\n",
    "            \n",
    "            # Step 2: Prepare metadata\n",
    "            metadata = self._load_metadata(filename)\n",
    "            self._validate_metadata(metadata)\n",
    "            \n",
    "            # Step 3: Database operations\n",
    "            table_name = self._sanitize_table_name(filename)\n",
    "            self._load_to_postgres(table_name, df)\n",
    "            \n",
    "            # Step 4: CKAN operations\n",
    "            dataset_id = self.ckan.create_update_dataset(metadata['dataset'])\n",
    "            if not dataset_id:\n",
    "                return False, \"Dataset creation failed\"\n",
    "            \n",
    "            resource_success = self.ckan.manage_resource(\n",
    "                dataset_id,\n",
    "                file_path,\n",
    "                metadata['resource']\n",
    "            )\n",
    "            \n",
    "            if not resource_success:\n",
    "                return False, \"Resource creation failed\"\n",
    "            \n",
    "            # Step 5: Finalize\n",
    "            self._move_processed_files(filename)\n",
    "            return True, \"Success\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Processing failed: {str(e)}\")\n",
    "            return False, str(e)\n",
    "\n",
    "    def _detect_encoding(self, file_path: str) -> str:\n",
    "        \"\"\"Advanced encoding detection.\"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw = f.read(10000)  # Sample for detection\n",
    "            result = chardet.detect(raw)\n",
    "        \n",
    "        encoding = result['encoding'] or 'utf-8'\n",
    "        # Handle common misdetections\n",
    "        if encoding.lower() in ['windows-1252', 'iso-8859-1']:\n",
    "            return 'cp1252'\n",
    "        return encoding\n",
    "\n",
    "    def _load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Robust CSV file loading with encoding detection and fallback.\"\"\"\n",
    "        # List of encodings to try (in order of likelihood)\n",
    "        encodings_to_try = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "        \n",
    "        # Try each encoding until one works\n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                self.logger.info(f\"Attempting to load {file_path} with encoding: {encoding}\")\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    encoding=encoding,\n",
    "                    on_bad_lines='warn',  # Skip bad lines instead of failing\n",
    "                    dtype='string'         # Use string dtype to avoid type inference issues\n",
    "                )\n",
    "                self.logger.info(f\"Successfully loaded {file_path} with encoding: {encoding}\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                self.logger.warning(f\"Failed to load {file_path} with encoding: {encoding}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error loading {file_path}: {str(e)}\")\n",
    "                raise ValueError(f\"Failed to load file: {str(e)}\")\n",
    "        \n",
    "        # If no encoding works, raise an error\n",
    "        raise ValueError(\n",
    "            f\"Failed to load {file_path} with encodings: {', '.join(encodings_to_try)}\"\n",
    "        )\n",
    "\n",
    "    def _validate_metadata(self, metadata: Dict) -> None:\n",
    "        \"\"\"Ensure metadata meets CKAN requirements.\"\"\"\n",
    "        required = ['owner_org', 'title']\n",
    "        missing = [field for field in required if not metadata['dataset'].get(field)]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required fields: {missing}\")\n",
    "\n",
    "        # Auto-fill common issues\n",
    "        if not metadata['dataset'].get('license_id'):\n",
    "            metadata['dataset']['license_id'] = 'uk-ogl'\n",
    "\n",
    "    def _sanitize_table_name(self, filename: str) -> str:\n",
    "        \"\"\"Generate a safe table name from filename.\"\"\"\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        return f\"datastore_{sanitize_name(base_name)}\"\n",
    "\n",
    "    def _load_to_postgres(self, table_name: str, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Load data into PostgreSQL.\"\"\"\n",
    "        try:\n",
    "            with psycopg2.connect(**self.config.db_params) as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    # Create table\n",
    "                    columns = [\n",
    "                        f\"{sanitize_name(col)} {self._infer_postgres_type(df[col].dtype)}\"\n",
    "                        for col in df.columns\n",
    "                    ]\n",
    "                    create_sql = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        {', '.join(columns)}\n",
    "                    )\n",
    "                    \"\"\"\n",
    "                    cur.execute(create_sql)\n",
    "                    \n",
    "                    # Insert data\n",
    "                    from io import StringIO\n",
    "                    output = StringIO()\n",
    "                    df.to_csv(output, sep='\\t', header=False, index=False)\n",
    "                    output.seek(0)\n",
    "                    cur.copy_from(output, table_name, null=\"\")\n",
    "                    conn.commit()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"PostgreSQL load failed: {str(e)}\")\n",
    "\n",
    "    def _infer_postgres_type(self, dtype) -> str:\n",
    "        \"\"\"Map pandas types to PostgreSQL types.\"\"\"\n",
    "        type_mapping = {\n",
    "            'int64': 'bigint',\n",
    "            'float64': 'double precision',\n",
    "            'object': 'text',\n",
    "            'datetime64[ns]': 'timestamp',\n",
    "            'bool': 'boolean'\n",
    "        }\n",
    "        return type_mapping.get(str(dtype), 'text')\n",
    "\n",
    "    def _load_metadata(self, filename: str) -> Dict:\n",
    "        \"\"\"Load and validate metadata file.\"\"\"\n",
    "        metadata_matches = [\n",
    "            f for f in os.listdir(self.config.pending_metadata_dir)\n",
    "            if f.startswith(f\"metadata_{os.path.splitext(filename)[0]}\")\n",
    "        ]\n",
    "        if not metadata_matches:\n",
    "            raise ValueError(\"No matching metadata file found\")\n",
    "        \n",
    "        latest_metadata = max(metadata_matches)\n",
    "        metadata_path = os.path.join(self.config.pending_metadata_dir, latest_metadata)\n",
    "        \n",
    "        try:\n",
    "            with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Metadata load failed: {str(e)}\")\n",
    "\n",
    "    def _move_processed_files(self, filename: str) -> None:\n",
    "        \"\"\"Move processed files to completed directories.\"\"\"\n",
    "        try:\n",
    "            # Move data file\n",
    "            src_file = os.path.join(self.config.pending_file_dir, filename)\n",
    "            dest_file = os.path.join(self.config.completed_file_dir, filename)\n",
    "            os.rename(src_file, dest_file)\n",
    "            \n",
    "            # Move metadata file\n",
    "            metadata_matches = [\n",
    "                f for f in os.listdir(self.config.pending_metadata_dir)\n",
    "                if f.startswith(f\"metadata_{os.path.splitext(filename)[0]}\")\n",
    "            ]\n",
    "            if metadata_matches:\n",
    "                latest_metadata = max(metadata_matches)\n",
    "                src_meta = os.path.join(self.config.pending_metadata_dir, latest_metadata)\n",
    "                dest_meta = os.path.join(self.config.completed_metadata_dir, latest_metadata)\n",
    "                os.rename(src_meta, dest_meta)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"File move failed: {str(e)}\")\n",
    "\n",
    "    def _get_pending_files(self) -> List[str]:\n",
    "        \"\"\"Get list of files to process.\"\"\"\n",
    "        return [\n",
    "            f for f in os.listdir(self.config.pending_file_dir)\n",
    "            if os.path.isfile(os.path.join(self.config.pending_file_dir, f))\n",
    "            and f.endswith((\".csv\", \".xlsx\"))\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = FileProcessor()\n",
    "    processor.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
